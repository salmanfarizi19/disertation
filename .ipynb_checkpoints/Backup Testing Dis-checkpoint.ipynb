{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5081d33-e746-4479-a0d8-7419d6a1ef49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8185d540-3892-476c-a56f-1ac9f0e35eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\tsalm\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9da1d2b-b2f0-46bc-aa16-729a6329a30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Share: JAKARTA - The digital footprint of the Governor of DKI Jakarta Anies Baswedan regarding the role of Arabs in recognizing the Indonesian homeland has gone viral on Twitter. This clip of Anies' lecture video was first uploaded by the @RD_4WR1212 account and has been watched by 2,973 as of Wednesday, October 27 at 18:43 WIB.  This video clip with a duration of 1.18 seconds was also replayed by social media activist, Ferdinand Hutahaean. Apart from Anies, the video also clearly shows the former high priest of the Islamic Defenders Front (FPI), Rizieq Shihab.  Anies explained that his family is the founder of the Indonesian Arab Party (PAI). In 1934, PAI had declared that their homeland was Indonesia.  \"Ladies and gentlemen, Arabs say that their homeland was Indonesia in 1934, what happened? Reckless! Year 34, reckless! Does Indonesia already exist? Not yet! Not yet!,\" Anies said in the video.  \"If a Javanese says I'm Indonesian, it turns out that Indonesia didn't happen, he's still Javanese. A Bugis says he's Indonesia, it turns out that Indonesia didn't happen, then he's still a Bugis,\" Anies continued.  So what will happen to the Arabs if Indonesia does not exist?  \"Now, the Arabs say that I am an Indonesian, the goods don't exist yet, if it turns out that Indonesia did not happen? If you want to admit that you are Javanese, you can't, if you want to admit that you are Arab, you say you have said Indonesia. Just look for Indonesia there, there are no items,\" explained Anies.  Twips Listen to the Lecture of the Former Minister of Education and Culture of the 2024.1 Indonesian Presidential Candidate, October 28, 1928, the Youth Pledge was implemented. Anies said that only Arabs declared an Indonesian homeland before Indonesia existed, no one did that except for Arab descendants in Indonesia. pic.twitter.com/eQUdbHRt0v  At the end of the video, Anies claims that only Arabs first swore an oath to the Indonesian homeland before Indonesia existed.  \"They swore an oath to the Indonesian homeland before Indonesia existed. No one did that except for Arab descendants in Indonesia,\" said Anies.  The account @RD_4WR1212 in a tweet said, \"Twips Listen to the Lecture of the Former Minister of Education and Culture of the 2024th Indonesian Presidential Candidate. 1 Day More October 28, 1928 Youth Pledge Executed. Anies Says Only Arabs declared Indonesia Homeland before Indonesia existed, No one did that except descendants Arabic in Indonesia.\"  While Ferdinand responded briefly to this old video clip. \"It's just a lie, nies,\" short Ferdinand on Twitter, @FerdinandHaean3. The English, Chinese, Japanese, Arabic, and French versions are automatically generated by the AI. So there may still be inaccuracies in translating, please always see Indonesian as our main language.\n",
      "                                    (system supported by DigitalSiber.id) \n",
      "                                                Tag:\n",
      "                                                                                                    twitter\n",
      "anies baswedan\n",
      "viral\n",
      "dki jakarta\n",
      " Â© 2024 VOI - Waktunya Merevolusi Pemberitaan \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the webpage to scrape\n",
    "url = 'https://voi.id/en/news/98472#google_vignette'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the content of the request with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Extract specific elements, e.g., all paragraph tags\n",
    "    paragraphs = soup.find_all('p')\n",
    "    text_data = ' '.join(p.text for p in paragraphs)  # Concatenate all paragraphs into a single string\n",
    "    print(text_data)\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e22dc22-7390-4252-b975-ebbe3a43c61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.021, 'neu': 0.898, 'pos': 0.081, 'compound': 0.998}\n"
     ]
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Initialize VADER sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Analyze the sentiment of the extracted text\n",
    "sentiment = analyzer.polarity_scores(text_data)\n",
    "print(sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7772624-b22b-439c-924d-b51c4a7c7dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_url(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            paragraphs = soup.find_all('p')\n",
    "            text_data = ' '.join(p.text for p in paragraphs)\n",
    "            return text_data\n",
    "        else:\n",
    "            return \"Failed to retrieve the webpage\"\n",
    "    except requests.RequestException as e:\n",
    "        return str(e)\n",
    "\n",
    "# Path to the CSV file that contains the URLs\n",
    "input_csv = 'Dataset.csv'\n",
    "output_csv = 'scraped_data.csv'\n",
    "\n",
    "# Read URLs from CSV\n",
    "urls = []\n",
    "with open(input_csv, mode='r', newline='', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)  # Skip the header if there is one\n",
    "    for row in reader:\n",
    "        urls.append(row[0])  # Assuming URLs are in the first column\n",
    "\n",
    "# Scrape each URL and collect the data\n",
    "data = []\n",
    "for url in urls:\n",
    "    result = scrape_url(url)\n",
    "    data.append([url, result])\n",
    "\n",
    "# Write the output to a new CSV file\n",
    "with open(output_csv, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['URL', 'Content'])  # Write header\n",
    "    writer.writerows(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3fce832-4e37-4ea1-b444-bf0603bbcd3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\tsalm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Ensure the VADER lexicon is downloaded\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "def scrape_url(url):\n",
    "    \"\"\"Scrape the webpage at the given URL and return the text data or an error.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            paragraphs = soup.find_all('p')\n",
    "            text_data = ' '.join(p.text for p in paragraphs)\n",
    "            return text_data, None  # Successfully scraped text, no error\n",
    "        else:\n",
    "            return None, f\"Failed to retrieve the webpage, status code: {response.status_code}\"\n",
    "    except requests.RequestException as e:\n",
    "        return None, str(e)  # Error during requests\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    \"\"\"Analyze the sentiment of the provided text using VADER and return the scores.\"\"\"\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    return sid.polarity_scores(text)\n",
    "\n",
    "# Specify the paths to your input and output CSV files\n",
    "input_csv = 'scraped_data.csv'  # Update this to the path of your input CSV file\n",
    "output_csv = 'scraped_data_with_sentiment.csv'  # Output CSV file path\n",
    "\n",
    "# Read URLs from the input CSV\n",
    "urls = []\n",
    "with open(input_csv, mode='r', newline='', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)  # Skip the header if there is one\n",
    "    for row in reader:\n",
    "        urls.append(row[0])  # Assuming URLs are in the first column\n",
    "\n",
    "# Process each URL, scrape the content, perform sentiment analysis, and collect the data\n",
    "data = []\n",
    "for url in urls:\n",
    "    text, error = scrape_url(url)\n",
    "    if text:  # If text was successfully scraped\n",
    "        sentiment = analyze_sentiment(text)\n",
    "        data.append([url, text, sentiment['neg'], sentiment['neu'], sentiment['pos'], sentiment['compound']])\n",
    "    else:  # If there was an error\n",
    "        data.append([url, error, None, None, None, None])\n",
    "\n",
    "# Write the results to the output CSV file\n",
    "with open(output_csv, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['URL', 'Content', 'Negative', 'Neutral', 'Positive', 'Compound'])  # Column headers\n",
    "    writer.writerows(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12378395-3358-49f7-a311-a7c2d35e2289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 URL  Negative  Neutral  \\\n",
      "0  https://www.theguardian.com/world/2024/apr/25/...     0.149    0.749   \n",
      "1     https://www.bbc.co.uk/news/world-asia-68271486     0.083    0.803   \n",
      "\n",
      "   Positive  Compound  \n",
      "0     0.102   -0.9782  \n",
      "1     0.114    0.9904  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data from CSV\n",
    "csv_file = 'scraped_data_with_sentiment.csv'  # Make sure this is the correct path to your CSV file\n",
    "data = pd.read_csv(csv_file)\n",
    "\n",
    "# Select only the URL and sentiment scores\n",
    "filtered_data = data[['URL', 'Negative', 'Neutral', 'Positive', 'Compound']]\n",
    "\n",
    "# Display the table\n",
    "print(filtered_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89e35e2-8e79-4cdb-8062-f509e06bc876",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "import time\n",
    "import random\n",
    "\n",
    "# List of proxies\n",
    "proxies = [\n",
    "    'http://proxy1.example.com:8080',\n",
    "    'http://proxy2.example.com:8080',\n",
    "    'http://proxy3.example.com:8080'\n",
    "]\n",
    "\n",
    "# Random User-Agent\n",
    "user_agents = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "]\n",
    "\n",
    "# Setup Firefox options\n",
    "options = webdriver.FirefoxOptions()\n",
    "options.headless = False  # Run in non-headless mode\n",
    "options.set_preference(\"dom.webdriver.enabled\", False)\n",
    "options.set_preference(\"useAutomationExtension\", False)\n",
    "options.set_preference(\"general.useragent.override\", random.choice(user_agents))\n",
    "\n",
    "# Setup proxy\n",
    "proxy = random.choice(proxies)\n",
    "options.set_preference('network.proxy.type', 1)\n",
    "options.set_preference('network.proxy.http', proxy)\n",
    "options.set_preference('network.proxy.http_port', int(proxy.split(':')[-1]))\n",
    "options.set_preference('network.proxy.ssl', proxy)\n",
    "options.set_preference('network.proxy.ssl_port', int(proxy.split(':')[-1]))\n",
    "\n",
    "# Setup Firefox driver\n",
    "driver = webdriver.Firefox(service=Service(GeckoDriverManager().install()), options=options)\n",
    "\n",
    "# Function to perform random mouse movements\n",
    "def random_mouse_movements():\n",
    "    actions = webdriver.ActionChains(driver)\n",
    "    for _ in range(random.randint(5, 15)):\n",
    "        actions.move_by_offset(random.randint(-10, 10), random.randint(-10, 10)).perform()\n",
    "        time.sleep(random.uniform(0.1, 0.3))\n",
    "\n",
    "# Open a webpage\n",
    "driver.get('https://www.reuters.com/world/asia-pacific/who-is-running-president-indonesias-2024-election-2023-04-21/')\n",
    "\n",
    "try:\n",
    "    # Wait up to 20 seconds for the element to be visible\n",
    "    content = WebDriverWait(driver, 20).until(\n",
    "        EC.visibility_of_element_located((By.CLASS_NAME, \"some_class\"))\n",
    "    ).text\n",
    "    print(content)\n",
    "    \n",
    "    # Perform random mouse movements\n",
    "    random_mouse_movements()\n",
    "    \n",
    "    # Add a random delay\n",
    "    time.sleep(random.uniform(2, 5))\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
