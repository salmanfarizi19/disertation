{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5081d33-e746-4479-a0d8-7419d6a1ef49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8185d540-3892-476c-a56f-1ac9f0e35eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\tsalm\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9da1d2b-b2f0-46bc-aa16-729a6329a30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "      Asia\n",
      "  \n",
      " Former university chancellor Anies Baswedan is campaigning for change in his bid to become Indonesia's next president. CNA follows him on his campaign trail. Former university chancellor Anies Baswedan is campaigning for change in his bid to become Indonesia's next president. CNA follows him on his campaign trail. \n",
      "\n",
      "      News Video Reports\n",
      "  \n",
      " \n",
      "\n",
      "      News Video Reports\n",
      "  \n",
      " \n",
      "\n",
      "      News Video Reports\n",
      "  \n",
      " \n",
      "\n",
      "      News Video Reports\n",
      "  \n",
      " \n",
      "\n",
      "      News Video Reports\n",
      "  \n",
      " \n",
      "\n",
      "      World\n",
      "  \n",
      " \n",
      "\n",
      "      News Video Reports\n",
      "  \n",
      " \n",
      "\n",
      "      Asia\n",
      "  \n",
      " \n",
      "\n",
      "      Watch\n",
      "  \n",
      " \n",
      "\n",
      "      Watch\n",
      "  \n",
      " CopyrightÂ© Mediacorp 2024. Mediacorp Pte Ltd. All rights reserved. We know it's a hassle to switch browsers but we want your experience with CNA to be fast, secure and the best it can possibly be. To continue, upgrade to a supported browser or, for the finest experience, download the mobile app. Upgraded but still having issues? Contact us\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the webpage to scrape\n",
    "url = '\\\n",
    "https://www.channelnewsasia.com/watch/anies-baswedans-campaign-indonesias-presidency-video-4016726'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the content of the request with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Extract specific elements, e.g., all paragraph tags\n",
    "    paragraphs = soup.find_all('p')\n",
    "    text_data = ' '.join(p.text for p in paragraphs)  # Concatenate all paragraphs into a single string\n",
    "    print(text_data)\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5e22dc22-7390-4252-b975-ebbe3a43c61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.116, 'neu': 0.766, 'pos': 0.118, 'compound': 0.2927}\n"
     ]
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Initialize VADER sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Analyze the sentiment of the extracted text\n",
    "sentiment = analyzer.polarity_scores(text_data)\n",
    "print(sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b7772624-b22b-439c-924d-b51c4a7c7dfb",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Read URLs from CSV\u001b[39;00m\n\u001b[0;32m     23\u001b[0m urls \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(input_csv, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m     25\u001b[0m     reader \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mreader(file)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28mnext\u001b[39m(reader)  \u001b[38;5;66;03m# Skip the header if there is one\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[1;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Dataset.csv'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_url(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            paragraphs = soup.find_all('p')\n",
    "            text_data = ' '.join(p.text for p in paragraphs)\n",
    "            return text_data\n",
    "        else:\n",
    "            return \"Failed to retrieve the webpage\"\n",
    "    except requests.RequestException as e:\n",
    "        return str(e)\n",
    "\n",
    "# Path to the CSV file that contains the URLs\n",
    "input_csv = 'Dataset.csv'\n",
    "output_csv = 'scraped_data.csv'\n",
    "\n",
    "# Read URLs from CSV\n",
    "urls = []\n",
    "with open(input_csv, mode='r', newline='', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)  # Skip the header if there is one\n",
    "    for row in reader:\n",
    "        urls.append(row[0])  # Assuming URLs are in the first column\n",
    "\n",
    "# Scrape each URL and collect the data\n",
    "data = []\n",
    "for url in urls:\n",
    "    result = scrape_url(url)\n",
    "    data.append([url, result])\n",
    "\n",
    "# Write the output to a new CSV file\n",
    "with open(output_csv, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['URL', 'Content'])  # Write header\n",
    "    writer.writerows(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3fce832-4e37-4ea1-b444-bf0603bbcd3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\tsalm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Ensure the VADER lexicon is downloaded\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "def scrape_url(url):\n",
    "    \"\"\"Scrape the webpage at the given URL and return the text data or an error.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            paragraphs = soup.find_all('p')\n",
    "            text_data = ' '.join(p.text for p in paragraphs)\n",
    "            return text_data, None  # Successfully scraped text, no error\n",
    "        else:\n",
    "            return None, f\"Failed to retrieve the webpage, status code: {response.status_code}\"\n",
    "    except requests.RequestException as e:\n",
    "        return None, str(e)  # Error during requests\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    \"\"\"Analyze the sentiment of the provided text using VADER and return the scores.\"\"\"\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    return sid.polarity_scores(text)\n",
    "\n",
    "# Specify the paths to your input and output CSV files\n",
    "input_csv = 'scraped_data.csv'  # Update this to the path of your input CSV file\n",
    "output_csv = 'scraped_data_with_sentiment.csv'  # Output CSV file path\n",
    "\n",
    "# Read URLs from the input CSV\n",
    "urls = []\n",
    "with open(input_csv, mode='r', newline='', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)  # Skip the header if there is one\n",
    "    for row in reader:\n",
    "        urls.append(row[0])  # Assuming URLs are in the first column\n",
    "\n",
    "# Process each URL, scrape the content, perform sentiment analysis, and collect the data\n",
    "data = []\n",
    "for url in urls:\n",
    "    text, error = scrape_url(url)\n",
    "    if text:  # If text was successfully scraped\n",
    "        sentiment = analyze_sentiment(text)\n",
    "        data.append([url, text, sentiment['neg'], sentiment['neu'], sentiment['pos'], sentiment['compound']])\n",
    "    else:  # If there was an error\n",
    "        data.append([url, error, None, None, None, None])\n",
    "\n",
    "# Write the results to the output CSV file\n",
    "with open(output_csv, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['URL', 'Content', 'Negative', 'Neutral', 'Positive', 'Compound'])  # Column headers\n",
    "    writer.writerows(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12378395-3358-49f7-a311-a7c2d35e2289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 URL  Negative  Neutral  \\\n",
      "0  https://www.theguardian.com/world/2024/apr/25/...     0.149    0.749   \n",
      "1     https://www.bbc.co.uk/news/world-asia-68271486     0.083    0.803   \n",
      "\n",
      "   Positive  Compound  \n",
      "0     0.102   -0.9782  \n",
      "1     0.114    0.9904  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data from CSV\n",
    "csv_file = 'scraped_data_with_sentiment.csv'  # Make sure this is the correct path to your CSV file\n",
    "data = pd.read_csv(csv_file)\n",
    "\n",
    "# Select only the URL and sentiment scores\n",
    "filtered_data = data[['URL', 'Negative', 'Neutral', 'Positive', 'Compound']]\n",
    "\n",
    "# Display the table\n",
    "print(filtered_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89e35e2-8e79-4cdb-8062-f509e06bc876",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "import time\n",
    "import random\n",
    "\n",
    "# List of proxies\n",
    "proxies = [\n",
    "    'http://proxy1.example.com:8080',\n",
    "    'http://proxy2.example.com:8080',\n",
    "    'http://proxy3.example.com:8080'\n",
    "]\n",
    "\n",
    "# Random User-Agent\n",
    "user_agents = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "]\n",
    "\n",
    "# Setup Firefox options\n",
    "options = webdriver.FirefoxOptions()\n",
    "options.headless = False  # Run in non-headless mode\n",
    "options.set_preference(\"dom.webdriver.enabled\", False)\n",
    "options.set_preference(\"useAutomationExtension\", False)\n",
    "options.set_preference(\"general.useragent.override\", random.choice(user_agents))\n",
    "\n",
    "# Setup proxy\n",
    "proxy = random.choice(proxies)\n",
    "options.set_preference('network.proxy.type', 1)\n",
    "options.set_preference('network.proxy.http', proxy)\n",
    "options.set_preference('network.proxy.http_port', int(proxy.split(':')[-1]))\n",
    "options.set_preference('network.proxy.ssl', proxy)\n",
    "options.set_preference('network.proxy.ssl_port', int(proxy.split(':')[-1]))\n",
    "\n",
    "# Setup Firefox driver\n",
    "driver = webdriver.Firefox(service=Service(GeckoDriverManager().install()), options=options)\n",
    "\n",
    "# Function to perform random mouse movements\n",
    "def random_mouse_movements():\n",
    "    actions = webdriver.ActionChains(driver)\n",
    "    for _ in range(random.randint(5, 15)):\n",
    "        actions.move_by_offset(random.randint(-10, 10), random.randint(-10, 10)).perform()\n",
    "        time.sleep(random.uniform(0.1, 0.3))\n",
    "\n",
    "# Open a webpage\n",
    "driver.get('https://www.reuters.com/world/asia-pacific/who-is-running-president-indonesias-2024-election-2023-04-21/')\n",
    "\n",
    "try:\n",
    "    # Wait up to 20 seconds for the element to be visible\n",
    "    content = WebDriverWait(driver, 20).until(\n",
    "        EC.visibility_of_element_located((By.CLASS_NAME, \"some_class\"))\n",
    "    ).text\n",
    "    print(content)\n",
    "    \n",
    "    # Perform random mouse movements\n",
    "    random_mouse_movements()\n",
    "    \n",
    "    # Add a random delay\n",
    "    time.sleep(random.uniform(2, 5))\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd95eb0c-c0c2-4fcf-ac9e-47dc77def949",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Plot sentiment distribution\u001b[39;00m\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m----> 5\u001b[0m data[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNegative\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNeutral\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPositive\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mplot(kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbar\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgray\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgreen\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOverall Sentiment Distribution\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSentiment\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot sentiment distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "data[['Negative', 'Neutral', 'Positive']].mean().plot(kind='bar', color=['red', 'gray', 'green'])\n",
    "plt.title('Overall Sentiment Distribution')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Mean Score')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a263ed4-2fe4-4fdc-972d-53fb758ba612",
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c597737e-d6d6-4b0f-a23b-409d6f71fa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the new CSV file\n",
    "file_path = csv_file+'_pivoted_table.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Truncate the URL to show only the first ten characters\n",
    "df['URL'] = df['URL'].str[:20]\n",
    "\n",
    "# Convert the dataframe to a pretty table\n",
    "pretty_table = tabulate(df, headers='keys', tablefmt='pretty')\n",
    "\n",
    "# Display the pretty table\n",
    "print(pretty_table)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
